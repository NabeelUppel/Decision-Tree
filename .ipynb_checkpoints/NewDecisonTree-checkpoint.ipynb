{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from anytree.importer import JsonImporter\n",
    "from anytree import RenderTree\n",
    "import copy\n",
    "import time\n",
    "import datetime\n",
    "Unseen =[]\n",
    "\n",
    "listOfUnseen=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class NpEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        else:\n",
    "            return super(NpEncoder, self).default(obj)\n",
    "\n",
    "def convert(d):\n",
    "    for k, v in d.items():\n",
    "        return { \"name\":k, \"children\": convert_helper(v)}\n",
    "\n",
    "def convert_helper(d):\n",
    "    if isinstance(d, dict):\n",
    "        return [{\"name\":k, \"children\":convert_helper(v)} for k, v in d.items()]\n",
    "    else:\n",
    "        if d==0.0:\n",
    "            d = \"<=50K\"\n",
    "        else:\n",
    "            d = \">50K\"\n",
    "        return [{\"name\": d}]\n",
    "\n",
    "def DecodedDict():\n",
    "    Education = {\n",
    "        0: \"Primary\",\n",
    "        1: \"Some High School\",\n",
    "        2: \"Grad High School\",\n",
    "        3: \"Some College\",\n",
    "        4: \"Bachelors\",\n",
    "        5: \"Masters\",\n",
    "        6: \"Doctorate\",\n",
    "        7: \"Professor - School\",\n",
    "        8: \"Associate - Academia\",\n",
    "        9: \"Associate - Voc\"\n",
    "    }\n",
    "\n",
    "    Gender = {\n",
    "        0: \"Male\",\n",
    "        1: \"Female\"\n",
    "    }\n",
    "\n",
    "    Race = {\n",
    "        0: \"Black\",\n",
    "        1: \"Asian-Pacific-Islander\",\n",
    "        2: \"Other\",\n",
    "        3: \"White\",\n",
    "        4: \"American-Indian-Eskimo\",\n",
    "    }\n",
    "\n",
    "    Occupation = {\n",
    "        1: \"Farm Fishing\",\n",
    "        2: \"Tech Support\",\n",
    "        3: \"Admin Clerical\",\n",
    "        4: \"Handlers Cleaners\",\n",
    "        5: \"Professor Speciality\",\n",
    "        6: \"Machine-Op Inspector\",\n",
    "        7: \"Exec Managerial\",\n",
    "        8: \"Private House Server\",\n",
    "        9: \"Craft - Repair\",\n",
    "        10: \"Sales\",\n",
    "        11: \"Transport-Moving\",\n",
    "        12: \"Armed Forces\",\n",
    "        13: \"Other Services\",\n",
    "        14: \"Protective Services\"\n",
    "    }\n",
    "\n",
    "    Hours = {\n",
    "        0: \"1-9\",\n",
    "        1: \"10-19\",\n",
    "        2: \"20-29\",\n",
    "        3: \"30-39\",\n",
    "        4: \"40-49\",\n",
    "        5: \"50-59\",\n",
    "        6: \"60-69\",\n",
    "        7: \"70-79\",\n",
    "        8: \"80-89\",\n",
    "        9: \"90-99\"\n",
    "    }\n",
    "\n",
    "    Age = {\n",
    "        0: \"17-19\",\n",
    "        1: \"20-29\",\n",
    "        2: \"30-39\",\n",
    "        3: \"40-49\",\n",
    "        4: \"50-59\",\n",
    "        5: \"60-69\",\n",
    "        6: \"70-79\",\n",
    "        7: \"80-90\",\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "    decodes = {\n",
    "        \"Education\": Education,\n",
    "        \"Gender\": Gender,\n",
    "        \"Race\": Race,\n",
    "        \"Occupation\": Occupation,\n",
    "        \"Hours\": Hours,\n",
    "        \"Age\": Age\n",
    "    }\n",
    "    return decodes\n",
    "\n",
    "def SplitData(dataFrame):\n",
    "    # Shuffles Dataset\n",
    "    # 60% Training\n",
    "    # 20% Testing\n",
    "    # 20% Validation\n",
    "\n",
    "    trainD, validateD, testD = np.split(dataFrame.sample(frac=1), [int(.6 * len(dataFrame)), int(.8 * len(dataFrame))])\n",
    "    trainD = trainD.reset_index(drop=True)\n",
    "    validateD = validateD.reset_index(drop=True)\n",
    "    testD = testD.reset_index(drop=True)\n",
    "    return trainD, validateD, testD\n",
    "\n",
    "def calcEntropy(target_col):\n",
    "    elements, counts = np.unique(target_col, return_counts=True)\n",
    "    entropy = []\n",
    "    for i in range(len(elements)):\n",
    "        c = counts[i]\n",
    "        total = np.sum(counts)\n",
    "        frac = c/total\n",
    "        log = np.log2(frac)\n",
    "        ent = -frac*log\n",
    "        entropy.append(ent)\n",
    "\n",
    "    return np.sum(entropy)\n",
    "\n",
    "def InfoGain(data, attribute_name, class_name=\"class\"):\n",
    "    total_entropy = calcEntropy(data[class_name])\n",
    "    totalSize = data.shape[0]\n",
    "\n",
    "    feature = data[attribute_name]\n",
    "    vals, counts = np.unique(feature, return_counts=True)\n",
    "\n",
    "    Weighted_Entropy = []\n",
    "    for i in range(len(vals)):\n",
    "        Class_Col = data.where(feature == vals[i]).dropna()[class_name]\n",
    "        ent = calcEntropy(Class_Col)\n",
    "        frac = (counts[i] / np.sum(counts))\n",
    "        prod = frac * ent\n",
    "        Weighted_Entropy.append(prod)\n",
    "\n",
    "    Weighted_Entropy = np.sum(Weighted_Entropy)\n",
    "\n",
    "    Information_Gain = total_entropy - (1/totalSize) * Weighted_Entropy\n",
    "    return Information_Gain\n",
    "\n",
    "def BuildTree(data, originalData, features, target_attribute_name=\"class\", parent_node_class=None, depth=0, max_depth = None):\n",
    "    Class_Col = data[target_attribute_name]\n",
    "    Orig_Class_Col = originalData[target_attribute_name]\n",
    "    depth +=1\n",
    "    # Stopping Conditions:\n",
    "    # if there is only one class in the table\n",
    "    if len(np.unique(Class_Col)) <= 1:\n",
    "        return np.unique(data[target_attribute_name])[0]\n",
    "\n",
    "    elif max_depth is not None and depth == max_depth:\n",
    "        return parent_node_class\n",
    "        #return np.unique(Orig_Class_Col)[np.argmax(np.unique(Orig_Class_Col, return_counts=True)[1])]\n",
    "\n",
    "    #if there is no data in the table, return the most common class\n",
    "    elif len(data) == 0:\n",
    "        return np.unique(Orig_Class_Col)[np.argmax(np.unique(Orig_Class_Col, return_counts=True)[1])]\n",
    "\n",
    "    # if all the features are used.\n",
    "    elif len(features) == 0:\n",
    "        return parent_node_class\n",
    "\n",
    "    # Build the tree.\n",
    "    else:\n",
    "        # the parent of the feature.\n",
    "        parent_node_class = np.unique(Class_Col)[np.argmax(np.unique(Class_Col, return_counts=True)[1])]\n",
    "\n",
    "        # Calculate the info gain for all the features.\n",
    "        item_values = [InfoGain(data, feature, target_attribute_name) for feature in features]\n",
    "\n",
    "        # Pick the items with the highest info gain (will be used to split the data)\n",
    "        best_feature_index = np.argmax(item_values)\n",
    "        root = features[best_feature_index]\n",
    "\n",
    "        tree = {root: {}}\n",
    "\n",
    "        # Remove the best feature from the data.\n",
    "        features = [i for i in features if i != root]\n",
    "\n",
    "        # Recursively build the tree.\n",
    "        for v in np.unique(data[root]):\n",
    "            v = v\n",
    "            sub_data = data.where(data[root] == v).dropna()\n",
    "            subtree = BuildTree(sub_data, originalData, features, target_attribute_name, parent_node_class,depth, max_depth)\n",
    "            tree[root][v] = subtree\n",
    "\n",
    "        return tree\n",
    "\n",
    "def TrainingDepth(train_data,feature_list):\n",
    "    depth_trees = {}\n",
    "    for i in range(2,8):\n",
    "        tree = BuildTree(train_data, train_data, feature_list,depth=-1, max_depth=i)\n",
    "        depth_trees[i] = tree\n",
    "    return depth_trees\n",
    "\n",
    "def TrainingFeatures(train_data,feature_list):\n",
    "    feat_size = len(feature_list)\n",
    "    feature_set = [feature_list]\n",
    "    for i in range(feat_size):\n",
    "        new_feat_set = copy.deepcopy(feature_list)\n",
    "        new_feat_set.pop(i)\n",
    "        feature_set.append(new_feat_set)\n",
    "\n",
    "    feat_dict = {}\n",
    "    for feat in feature_set:\n",
    "        feat_dict[tuple(feat)] =  BuildTree(train_data, train_data, feat)\n",
    "\n",
    "    return feat_dict\n",
    "\n",
    "def Training(data,feature_list):\n",
    "    tree = BuildTree(data, data, feature_list)\n",
    "    return tree\n",
    "\n",
    "def TreeErrors(depth_trees, valid_data):\n",
    "    depth_errors ={}\n",
    "    for k,v in depth_trees.items():\n",
    "        curr_err =calcErrors(valid_data, v)[0][\"Error\"]\n",
    "        depth_errors[k] = curr_err\n",
    "\n",
    "    return min(depth_errors, key=lambda k1: depth_errors[k1])\n",
    "\n",
    "def predict(query,tree,default = 0):\n",
    "    for k in list(query.keys()):\n",
    "        if k in list(tree.keys()):\n",
    "            try:\n",
    "                result = tree[k][query[k]]\n",
    "                if isinstance(result,dict):\n",
    "                    return predict(query,result)\n",
    "\n",
    "                else:\n",
    "                    return result\n",
    "            except KeyError:\n",
    "                Unseen.append(query)\n",
    "                return default\n",
    "\n",
    "def testing(data,tree):\n",
    "    Unseen.clear()\n",
    "    labels = data.copy().pop(data.columns[-1])\n",
    "    queries = data.iloc[:,:-1].to_dict(orient = \"records\")\n",
    "    predictions = pd.DataFrame(columns=[\"predicted\"])\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        predictions.loc[i,\"predicted\"] = predict(queries[i],tree,0.0)\n",
    "    predictions = predictions.join(labels)\n",
    "    predictions = predictions.astype(int)\n",
    "    return predictions\n",
    "\n",
    "def tree2JSON(tree, fileName):\n",
    "    with open(fileName+\".json\", \"w\") as f:\n",
    "        json.dump(convert(tree), fp=f, indent=2, cls=NpEncoder)\n",
    "\n",
    "def treeViewer(fileName, Print=False, ToFile=False):\n",
    "    importer = JsonImporter()\n",
    "    with open(fileName+'.json',\"r\") as js:\n",
    "        data = js.read()\n",
    "    root = importer.import_(data)\n",
    "    if Print:\n",
    "        if ToFile:\n",
    "            file = open(fileName+\".txt\",\"w+\")\n",
    "            for pre, fill, node in RenderTree(root):\n",
    "                print(\"%s%s\" % (pre, node.name), file=file)\n",
    "\n",
    "        else:\n",
    "             for pre, fill, node in RenderTree(root):\n",
    "                print(\"%s%s\" % (pre, node.name))\n",
    "    return root\n",
    "\n",
    "def confusionMatrix(predicted, actual):\n",
    "    df_confusion = pd.crosstab(actual, predicted, rownames=['Actual'], colnames=['Predicted'], dropna=False)\n",
    "    return df_confusion\n",
    "\n",
    "def calcEvalMetrics(cm):\n",
    "    TP = cm[0][0]\n",
    "    FP = cm[0][1]\n",
    "    FN = cm[1][0]\n",
    "    TN = cm[1][1]\n",
    "    Total  = TP+FP+FN+TN\n",
    "    error = (FP +FN)/Total\n",
    "    accuracy = (TP+TN)/Total\n",
    "    falseAlarm = FP/(FP+TN)\n",
    "    miss = FN/(TP+FN)\n",
    "    recall = TP/(TP+FN)\n",
    "    precision = TP/(TP+FP)\n",
    "    metrics =  {\"Error\":error,\n",
    "                \"Accuracy\": accuracy,\n",
    "                \"False Alarm\": falseAlarm,\n",
    "                \"Miss\": miss,\n",
    "                \"Recall\": recall,\n",
    "                \"Precision\":precision\n",
    "                }\n",
    "    return metrics\n",
    "\n",
    "def calcErrors(data,tree):\n",
    "    df = testing(data, tree)\n",
    "    predicted= df[df.columns[0]].to_numpy()\n",
    "    actual = df[df.columns[1]].to_numpy()\n",
    "    cm = confusionMatrix(predicted, actual)\n",
    "    metrics = calcEvalMetrics(cm)\n",
    "    return metrics, cm\n",
    "\n",
    "def PrintMetrics(unseen, metrics, cm, title,File=None, ToFile=False,hyperParams =None):\n",
    "    mostIssues(unseen)\n",
    "    if ToFile:\n",
    "        f = File\n",
    "        print(title, file=f)\n",
    "        print(\"Note: O:  <=50K and 1: >50K \\n\",file=f)\n",
    "\n",
    "        if hyperParams is not None:\n",
    "            print(\"HyperParameters:\",file=f)\n",
    "            print(\"Features List:\", hyperParams[0],file=f)\n",
    "            print(\"Max Depth:\", hyperParams[1],file=f)\n",
    "            print(\"\\n\",file=f)\n",
    "\n",
    "        print(\"No. of Unseen Data Points: \",len(unseen), \"(Default is 0)\",file=f)\n",
    "\n",
    "        print(\"\\n\",file=f)\n",
    "        print(\"Confusion Matrix:\",file=f)\n",
    "        print(cm,file=f)\n",
    "\n",
    "        print(\"\\n\",file=f)\n",
    "        print(\"Metrics:\",file=f)\n",
    "        for k,v in metrics.items():\n",
    "            print(\"{0:15} {1}\".format(k,v),file=f)\n",
    "        print(\"\\n\", file=f)\n",
    "    else:\n",
    "        print(title)\n",
    "        print(\"Note: O:  <=50K and 1: >50K \\n\")\n",
    "\n",
    "        if hyperParams is not None:\n",
    "            print(\"HyperParameters:\")\n",
    "            print(\"Features List:\", hyperParams[0])\n",
    "            print(\"Max Depth:\", hyperParams[1])\n",
    "            print(\"\\n\")\n",
    "\n",
    "        print(\"No. of Unseen Data Points: \",len(unseen), \"(Default is 0)\")\n",
    "\n",
    "        print(\"\\n\")\n",
    "        print(\"Confusion Matrix:\")\n",
    "        print(cm)\n",
    "\n",
    "        print(\"\\n\")\n",
    "        print(\"Metrics:\")\n",
    "        for k,v in metrics.items():\n",
    "            print(\"{0:15} {1}\".format(k,v))\n",
    "        print(\"\\n\")\n",
    "\n",
    "def PrintResults(test_data, old_tree, new_tree, hyperParams, FileName =\"Results\", ToFile=False ):\n",
    "    listOfUnseen.clear()\n",
    "\n",
    "\n",
    "    if ToFile:\n",
    "        File = open(FileName+\".txt\", \"w+\")\n",
    "        title1 = \"Without HyperParameters Tuning:\"\n",
    "        old_Metrics, old_CM = calcErrors(test_data, old_tree)\n",
    "        PrintMetrics(Unseen, old_Metrics, old_CM, title1, File, ToFile,hyperParams=(feats, \"None\"))\n",
    "        title2 = \"With HyperParameters Tuning:\"\n",
    "        new_Metrics, new_CM = calcErrors(test_data, new_tree)\n",
    "        PrintMetrics(Unseen, new_Metrics, new_CM, title2,File, ToFile, hyperParams=hyperParams)\n",
    "        File.close()\n",
    "    else:\n",
    "        title1 = \"Without HypeParameters Tuning:\"\n",
    "        old_Metrics, old_CM = calcErrors(test_data, old_tree)\n",
    "        PrintMetrics(Unseen, old_Metrics,  old_CM,title1, hyperParams=(feats, \"None\"))\n",
    "\n",
    "        title2 = \"With HyperParameters Tuning:\"\n",
    "        new_Metrics, new_CM = calcErrors(test_data, new_tree)\n",
    "        PrintMetrics(Unseen, new_Metrics, new_CM, title2,hyperParams=hyperParams)\n",
    "\n",
    "def printUnseen(uS):\n",
    "    stats={}\n",
    "    for k,v in uS[0].items():\n",
    "        stats[k]={}\n",
    "\n",
    "\n",
    "    for l in uS:\n",
    "        for k,v in l.items():\n",
    "            key_dict = stats[k]\n",
    "            if v not in key_dict.keys():\n",
    "                key_dict[v] = 0\n",
    "            else:\n",
    "                key_dict[v] += 1\n",
    "    return stats\n",
    "\n",
    "def statsView(stats):\n",
    "    vals = {}\n",
    "    for k,v in stats.items():\n",
    "        #print(k)\n",
    "        max_key = max(v, key=lambda k2: v[k2])\n",
    "        for k1,v1 in v.items():\n",
    "            #print(k1, v1)\n",
    "            pass\n",
    "        print(max_key, v[max_key])\n",
    "        vals[k] = max_key\n",
    "    print(\"\\n\")\n",
    "    return vals\n",
    "def mostIssues(unseen):\n",
    "    stats1 = printUnseen(unseen)\n",
    "    struggle1 = statsView(stats1)\n",
    "    print(len(unseen),struggle1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "DataSet = pd.read_csv(\"income_dataset.csv\")\n",
    "DataSet.rename(columns={\"education\":\"Education\",\"age\":\"Age\",\"race\":\"Race\",\"gender\":\"Gender\",\"occupation\":\"Occupation\",\"hours per week\": \"Hours\", \"income\": \"class\"}, inplace=True)\n",
    "values = DecodedDict()\n",
    "\n",
    "for key, value in values.items():\n",
    "    DataSet[key] = DataSet[key].map(value)\n",
    "\n",
    "train, validate, test = SplitData(DataSet.copy())\n",
    "feats = list(train.columns[:-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Taken With Tuning:  --- 438.8210184574127 seconds ---\n",
      "Time Taken With Tuning:  0:07:18.821247\n"
     ]
    }
   ],
   "source": [
    "tuningStart_time = time.time()\n",
    "# Hyperparameter Tuning:\n",
    "# Find tree that will give lowest error when you remove a single feature from list\n",
    "feat_trees = TrainingFeatures(train, feats)\n",
    "best_feat_set = list(TreeErrors(feat_trees, validate))\n",
    "\n",
    "# Find the best tree that will give the lowest error when changing the max depth the tree can grow to.\n",
    "depth_Trees = TrainingDepth(train, best_feat_set)\n",
    "best_depth_tree = TreeErrors(depth_Trees, validate)\n",
    "\n",
    "# Best Tree\n",
    "Tree = depth_Trees[best_depth_tree]\n",
    "tree2JSON(Tree, \"Tree(Tuning)\")\n",
    "TreeStruct = treeViewer(\"Tree(Tuning)\",Print=True,ToFile=True)\n",
    "hyperParamsSet = (best_feat_set, best_depth_tree)\n",
    "print(\"Time Taken With Tuning: \",\"--- %s seconds ---\" % (time.time() - tuningStart_time))\n",
    "print(\"Time Taken With Tuning: \",str(datetime.timedelta(seconds=(time.time() - tuningStart_time))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Time Taken Without Tuning:  --- 68.07447648048401 seconds ---\n",
      "Time Taken Without Tuning:  0:01:08.074698\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\")\n",
    "Start_time = time.time()\n",
    "# Tree Without HyperParameter Tuning\n",
    "oldTree = Training(train, feats)\n",
    "tree2JSON(oldTree, \"Tree\")\n",
    "OldTreeStruct = treeViewer(\"Tree\",Print=True,ToFile=True)\n",
    "print(\"Time Taken Without Tuning: \",\"--- %s seconds ---\" % (time.time() - Start_time))\n",
    "print(\"Time Taken Without Tuning: \",str(datetime.timedelta(seconds=(time.time() - Start_time))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40-49 90\n",
      "Grad High School 74\n",
      "Professor Speciality 51\n",
      "White 233\n",
      "Male 238\n",
      "40-49 113\n",
      "\n",
      "\n",
      "361 {'Age': '40-49', 'Education': 'Grad High School', 'Occupation': 'Professor Speciality', 'Race': 'White', 'Gender': 'Male', 'Hours': '40-49'}\n",
      "60-69 18\n",
      "Grad High School 20\n",
      "Farm Fishing 10\n",
      "White 33\n",
      "Male 47\n",
      "60-69 9\n",
      "\n",
      "\n",
      "67 {'Age': '60-69', 'Education': 'Grad High School', 'Occupation': 'Farm Fishing', 'Race': 'White', 'Gender': 'Male', 'Hours': '60-69'}\n"
     ]
    }
   ],
   "source": [
    "PrintResults(test, oldTree, Tree, hyperParamsSet, ToFile=True)\n",
    "#PrintResults(test, oldTree, Tree, hyperParamsSet, ToFile=False)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Age           2314\n",
      "Education     2314\n",
      "Occupation    2314\n",
      "Race          2314\n",
      "Gender        2314\n",
      "Hours         2314\n",
      "class         2314\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(train[(train['Occupation']==\"Professor Speciality\")].count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
